# SPA Benchmark Suite

## Overview
The SPA Benchmark Suite compares the runtime behaviour of three single-page applications (SPA) implemented with **React**, **Vue**, and **Svelte**. Each application reproduces the same data workflows—dashboard KPIs, a paginated catalog with filtering and stress testing, CRUD forms, and an about page—to keep the benchmarks focused on framework-level differences. The goal is to capture functional parity so that performance measurements highlight the impact of the rendering model rather than application logic.

## Repository structure
```
react-app/     React implementation of the benchmark SPA (Vite + React Router + Zustand).
vue-app/       Vue implementation (Vite + Vue Router + Pinia) mirroring the React features.
svelte-app/    Svelte implementation (Vite + Svelte stores) aligned with the same routes and flows.
scripts/       Automation utilities to launch builds, run browser benchmarks, and collate metrics.
data/          Shared JSON datasets consumed by the mock APIs in every framework.
docs/          Design notes, parity specifications, and UI guidelines for the suite.
results/       Git-ignored folder generated by the benchmark scripts to store raw and aggregated data.
```

## Installing the applications
All projects target the current **Node.js LTS** release. Install dependencies independently for each framework:

### React app (`react-app/`)
```bash
cd react-app
npm install
npm run dev          # starts Vite dev server on port 5173 by default
npm run build        # creates the production build under dist/
npm run preview      # serves the production bundle locally
npm run start:prod   # convenience script = build + preview
```

### Vue app (`vue-app/`)
```bash
cd vue-app
npm install
npm run dev          # starts Vite dev server on port 5174 by default
npm run build
npm run preview
npm run start:prod
```

### Svelte app (`svelte-app/`)
```bash
cd svelte-app
npm install
npm run dev          # starts Vite dev server (default port 5175)
npm run build
npm run preview
npm run start:prod
```

Run `npm run lint` in any application to execute the stack-specific ESLint configuration before shipping changes.

## Running the benchmarks
The `scripts/` workspace bundles the automation flows that open the SPAs in a controlled browser, collect metrics, and summarise the data. Typical workflow:

1. **Install tooling**
   ```bash
   cd scripts
   npm install
   ```

2. **Execute benchmark suites**
   - `npm run bench:react`
   - `npm run bench:vue`
   - `npm run bench:svelte`
   - `npm run bench:all` runs the three suites sequentially for easy comparisons.

   The scripts can target either production builds or dev servers. By default they spawn the production preview for each framework, load scenarios (dashboard render, catalog navigation, stress test, form submission), and capture timings through the browser Performance API plus the apps’ custom hooks.

3. **Post-process results**
   ```bash
   npm run results:build
   ```
   This aggregates raw JSON traces into CSV/Markdown summaries and produces comparison charts.

### Output location
Benchmark runs populate the `results/` directory:
```
results/
  raw/              Individual JSON dumps per run and scenario.
  summaries/        Aggregated metrics (CSV + Markdown) keyed by timestamp.
  reports/          Generated visualisations such as charts or HTML dashboards.
```
Each subfolder uses ISO-like timestamps (`2024-03-18T21-15-00Z/`) so that runs are easy to compare chronologically.

## Tips for consistent measurements
- Close unrelated desktop applications and browser tabs before running a suite.
- Use the **same browser build** for every framework run; prefer Chromium-based browsers launched with a clean profile.
- Disable automatic updates, background sync, and notifications in the benchmark browser profile.
- Pin the CPU governor (e.g., `performance` on Linux or `High Performance` on Windows) and keep the device plugged into power.
- Clear browser caches between runs (`--incognito` or scripted `page.goto` with cache disabled`).
- Avoid touching the mouse/keyboard while scenarios execute to prevent extra painting work.
- Repeat each benchmark at least three times and rely on medians instead of single runs.
- Capture the Git commit hash alongside every exported result so that code changes are traceable.
